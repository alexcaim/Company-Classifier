# Company-Classifier
Project in Python based on the correct classification of businesses using unsupervised learning techniques from ML.

The problem we are trying to solve is not limited to simple clustering, because in addition to grouping businesses, they also need to be assigned to the correct insurance. Initially, I tried an approach based on embedding and fuzzy k-means, which also incorporated some aspects of k-nearest-centroid. In short, we fixed the labels as centroids and tried to find the probability of each point's membership to these centroids, without moving them as would normally happen in k-means. The issue here was with the embedding part, because if we want to bring these points into the same space, we need to find something applicable to both the business data and the insurance names. The more constraints we impose on these embeddings, the more useless the subsequent algorithm becomes.

The idea that proved to be much better was to no longer look for values for points but for distances. The distance from the point to the centroid represents the level of match between the information about a business and the name of the insurance. Thus, we need a function that takes two lists of strings as input and returns the discrepancy between them. I first tried the minimum Levenshtein distance, but it doesn't take word length into account, only how many characters differ. (There are words that have a common domain because of the prefix, but when one letter out of three is incorrect, it's clear that there's no connection between the terms.) Thus, I implemented a function that also takes into account the length of the terms. The final distance is the distance calculated by the function between each term in the insurance name and the business data, and this time we are no longer interested in the spatial dimension, only the value, which is always positive, so it can be considered as a distance.

As a result of the change in approach, the algorithm was also changed. We use Expectation Maximization with weighted updates to obtain from the distance matrix a matrix of the same dimensions with membership probabilities. The algorithm tries to maximize the probabilities by moving the centroids. We don't have an issue with the movement in this case because the number of clusters remains constant.

If we pay attention to the dataset, we will notice that each business is defined by aspects with different levels of importance: low level (description) and normal level (tags, sector, etc.). We assign this level based on relevance in classification, considering the amount of data/words and the logic in the final response (certain selected terms do not necessarily define the business the same way as the other columns do). An improvement made was the addition of a weight for the words obtained from the description column.

The results and graphs are presented in the notebook along with the conclusions. A future improvement could be finding a correlation between businesses with common tags and adding it to the distance matrix. The issue that arises is whether the computational effort is worth it, since we are talking about a matrix 40 times larger than the current one. The current solution took an estimated 10-15 minutes to run on a local device, and ~90% of the time was spent on creating the distance matrix. If it is saved, adding new rows and columns is a formality in terms of time.
